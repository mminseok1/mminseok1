# -*- coding: utf-8 -*-
"""ANN

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gfJDal5yQGiDra1wBeMSbNiXfH35fmfq

#이진분류
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt



import tensorflow as tf

!gdown https://raw.githubusercontent.com/devdio/flyai_datasets/main/diabetes.csv

diabetes = pd.read_csv('diabetes.csv')

diabetes

diabetes.isna().sum()

diabetes.describe().T

cols = ['Glucose', 'BloodPressure','SkinThickness', 'Insulin', 'BMI']

diabetes[cols] = diabetes[cols].replace(0,np.nan)

diabetes.isna().sum()

for col in cols:
  diabetes[col] = diabetes[col].fillna(diabetes[col].mean())

diabetes.isna().sum()

X = diabetes.iloc[:,:-1]
Y = diabetes.iloc[:,-1]

from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0, stratify = Y)

X_train.shape , Y_train.shape , X_test.shape, Y_test.shape

Y_test.value_counts()

from sklearn.preprocessing import RobustScaler  # 4분위수를 이용해서 표준화함, 이상치에 강건함
RS = RobustScaler()
X_train = RS.fit_transform(X_train)
X_test = RS.transform(X_test)

X_train.shape

from tensorflow import keras
from keras import layers

model = keras.Sequential([
    layers.Dense(units = 32, activation = 'relu', input_shape = (8,)),
    layers.Dense(units = 8, activation = 'relu'),
    layers.Dense(units = 8, activation = 'relu'),
    layers.Dense(units = 1, activation = 'sigmoid') #이진분류는 출력이 1개, 2개도 가능함 -> Sigmoid 처리를 해줘야함
])

model.summary()

model.compile(
    optimizer = 'adam',
    loss = 'binary_crossentropy',
    metrics = ['accuracy'],
)

Epochs = 50
Batch_size = 32 # 사이즈가 작으면 W,b 갱신이 많음

History = model.fit(
    X_train,Y_train,
    epochs = Epochs,
    batch_size = Batch_size,
    validation_split = 0.2,
    verbose = 1
)

#val_loss가 떨어지는 방향으로 선택하는게 좋음

def plot_history(History):
    hist = pd.DataFrame(History.history)
    hist['epoch']= History.epoch

    plt.figure(figsize=(16, 8))
    plt.subplot(1, 2, 1)
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.plot(hist['epoch'], hist['loss'], label='Train Loss')
    plt.plot(hist['epoch'], hist['val_loss'], label='Val Loss')

    plt.subplot(1, 2, 2)
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.plot(hist['epoch'], hist['accuracy'], label='Train Accuracy')
    plt.plot(hist['epoch'], hist['val_accuracy'], label='Val Accuracy')

    plt.legend()
    plt.show()

plot_history(History)
#overfitting을 의심할 수 있음

#평가
Y_pred = model.predict(X_test)

Y_pred[:10]

# Sigmoid 값을 Labeling 해야함
Y_pred = (Y_pred > 0.5).astype('int').reshape(-1)

Y_test = Y_test.values

from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay
acc = accuracy_score(Y_test,Y_pred)
cm = confusion_matrix(Y_test, Y_pred)

disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot()
plt.show()





"""#다중분류"""

!gdown https://raw.githubusercontent.com/devdio/flyai_datasets/main/iris.csv

iris = pd.read_csv('iris.csv')

iris.head()

iris.isna().sum()

iris.describe().T

from sklearn.preprocessing import OneHotEncoder
OHE = OneHotEncoder(sparse = False)
OHE.fit(iris[['Species']])
iris[iris['Species'].unique()] = OHE.transform(iris[['Species']])

iris

X = iris.iloc[:,[1,2,3,4]]
Y = iris.iloc[:,[-3,-2,-1]]

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0, stratify = Y)

RS = RobustScaler()
X_train = RS.fit_transform(X_train)
X_test = RS.transform(X_test)

model = keras.Sequential()
model.add(layers.Dense(units = 64, activation = 'relu', input_shape = (4,)))
model.add(layers.Dense(units = 16, activation = 'relu',))
model.add(layers.Dense(units = 3, activation = 'softmax'))

model.compile(
    optimizer = 'adam',
    loss = 'categorical_crossentropy',
    metrics = 'accuracy'
)

History = model.fit(
    X_train,Y_train,
    epochs = 100,
    batch_size = 32,
    validation_split = 0.2,
    verbose = 1
)

def plot_history(History):
    hist = pd.DataFrame(History.history)
    hist['epoch']= History.epoch

    plt.figure(figsize=(16, 8))
    plt.subplot(1, 2, 1)
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.plot(hist['epoch'], hist['loss'], label='Train Loss')
    plt.plot(hist['epoch'], hist['val_loss'], label='Val Loss')

    plt.subplot(1, 2, 2)
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.plot(hist['epoch'], hist['accuracy'], label='Train Accuracy')
    plt.plot(hist['epoch'], hist['val_accuracy'], label='Val Accuracy')

    plt.legend()
    plt.show()

plot_history(History)

Y_pred = model.predict(X_test)

Y_pred[:5]

Y_test[:5]

Y_pred = np.argmax(Y_pred, axis = 1) # 가장 큰 값을 인덱스로 돌려줌

Y_test = Y_test.values

Y_test

Y_test = np.argmax(Y_test, axis = 1)

from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay
acc = accuracy_score(Y_test,Y_pred)
cm = confusion_matrix(Y_test, Y_pred)

disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot()
plt.show()





"""#다중분류"""

!gdown https://raw.githubusercontent.com/devdio/flyai_datasets/main/penguins.csv

penguins = pd.read_csv('penguins.csv', na_values = ['.'])

penguins.head()

penguins.isna().sum()

penguins.shape

penguins = penguins.dropna()

penguins.shape

penguins.describe().T

penguins.columns

penguins['species'].unique(), penguins['island'].unique(), penguins['sex'].unique()

from sklearn.preprocessing import LabelEncoder
LE = LabelEncoder()
penguins['species'] = LE.fit_transform(penguins['species'])
penguins['sex'] = LE.fit_transform(penguins['sex'])

penguins

from sklearn.preprocessing import OneHotEncoder
OHE = OneHotEncoder(sparse = False )
OHE.fit(penguins[['island']])
penguins[penguins['island'].unique()] = OHE.transform(penguins[['island']])

penguins

from sklearn.preprocessing import RobustScaler
RE = RobustScaler()
penguins[['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']] = RE.fit_transform(penguins[['culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g']])

penguins

X = penguins.iloc[:,2:]
Y = penguins.iloc[:,0]

X.shape, Y.shape

model = keras.Sequential()
model.add(layers.Dense(units = 64, activation = 'relu', input_shape = (8,)))
model.add(layers.Dense(units = 32, activation = 'relu'))
model.add(layers.Dense(units = 3, activation = 'softmax'))

model.compile(
    optimizer = 'adam',
    loss = 'sparse_categorical_crossentropy',
    metrics = 'accuracy'
)

History = model.fit(
    X,Y,
    epochs = 100,
    batch_size = 32,
    validation_split = 0.2,
    verbose = 1
)

Y_pred = model.predict(X)

Y_pred = np.argmax(Y_pred, axis = 1)

Y_pred

Y = Y.values

Y

acc = accuracy_score(Y_pred, Y)
acc

def plot_history(History):
    hist = pd.DataFrame(History.history)
    hist['epoch']= History.epoch

    plt.figure(figsize=(16, 8))
    plt.subplot(1, 2, 1)
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.plot(hist['epoch'], hist['loss'], label='Train Loss')
    plt.plot(hist['epoch'], hist['val_loss'], label='Val Loss')

    plt.subplot(1, 2, 2)
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.plot(hist['epoch'], hist['accuracy'], label='Train Accuracy')
    plt.plot(hist['epoch'], hist['val_accuracy'], label='Val Accuracy')

    plt.legend()
    plt.show()

plot_history(History)

